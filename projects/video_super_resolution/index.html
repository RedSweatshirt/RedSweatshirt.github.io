<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Frame Recurrent Video Super Resolution | John Eastman </title> <meta name="author" content="John Eastman"> <meta name="description" content="MIT Advances in Computer Vision (6.869) Final Project"> <meta name="keywords" content="portfolio, portfolio-website, john-eastman, jack-eastman, mit, massachusetts-institute-of-technology, computer-science, computer-graphics, computer-vision, software-engineer, software-developer,"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="/assets/libs/mdb/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="/assets/libs/google_fonts/google-fonts.css"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8D%81&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://redsweatshirt.github.io/projects/video_super_resolution/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <link defer rel="stylesheet" href="/assets/libs/img-comparison-slider/styles.min.css" integrity="sha256-3qTIuuUWIFnnU3LpQMjqiXc0p09rvd0dmj+WkpQXSR8=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/libs/swiper/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> John Eastman </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Frame Recurrent Video Super Resolution</h1> <p class="post-description">MIT Advances in Computer Vision (6.869) Final Project</p> </header> <article> <div class="container"> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/lowres-480.webp 480w,/assets/img/vsr_6869/lowres-800.webp 800w,/assets/img/vsr_6869/lowres-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/lowres.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Low Res Frame" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-5 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/RAFRVSR-480.webp 480w,/assets/img/vsr_6869/RAFRVSR-800.webp 800w,/assets/img/vsr_6869/RAFRVSR-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/RAFRVSR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Upscaled Frame" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> First Image: A frame from a low resolution video. Second Image: The same upscaled frame using my model. </div> <p>Enhancing Video Quality Using Advanced Neural Networks In the pursuit of enhancing video quality, I took on an exciting project for my class Advances in Computer Vision (6.869) at MIT in Spring 2022. The project focused on improving the <a href="https://arxiv.org/pdf/1801.04590.pdf" rel="external nofollow noopener" target="_blank">Frame Recurrent Video Super Resolution</a> (FRVSR) neural network by implementing perceptual loss and incorporating the state-of-the-art <a href="https://arxiv.org/pdf/2003.12039.pdf" rel="external nofollow noopener" target="_blank">RAFT</a> optical flow network.</p> <h4 id="motivation">Motivation</h4> <p>Super-resolution is the process of transforming a low-resolution image or video into a higher-resolution output. This technique is essential in various applications, such as restoring old films, enhancing streaming video quality, and improving the visuals of video games. Traditional methods, like bicubic up-sampling, tend to smooth out textures and edges, causing a loss of high-frequency details. More recent approaches, such as convolutional neural networks (CNNs) and Generative Adversarial Networks (GANs), have shown promising results, but often come with higher computational costs.</p> <p>The idea behind FRVSR is to use a recurrent network to implement video super-resolution, providing more temporal consistency and spatial awareness of content in a video. FRVSR utilizes SRNet as the super resolution network block, incorporated with FlowNet to The goal of this project was to enhance the FRVSR network by introducing perceptual loss and replacing the existing FlowNet with the more advanced RAFT network. This would allow the network to focus on high-level content losses while maintaining spatial and temporal consistency.</p> <h4 id="project-overview">Project Overview</h4> <div class="container"> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/FRVSRModel-480.webp 480w,/assets/img/vsr_6869/FRVSRModel-800.webp 800w,/assets/img/vsr_6869/FRVSRModel-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/FRVSRModel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FRVSR Model Diagram" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> The FRVSR Model. Image source: FRVSR paper </div> <div class="container"> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/RAFTModel-480.webp 480w,/assets/img/vsr_6869/RAFTModel-800.webp 800w,/assets/img/vsr_6869/RAFTModel-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/RAFTModel.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RAFT Model Diagram" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> RAFT Flow Network Model. Image source: RAFT paper </div> <p>To achieve this goal, I started with an implementation of the original FRVSR model and updated it, fixing errors and retraining the network multiple times with revised loss functions. I also incorporated a pre-trained implementation of RAFT and rewrote the FRVSR network to work seamlessly with it.</p> <p>The improved FRVSR network architecture consists of an SRNet super-resolution block and an optical flow network block (RAFT). The previous frame is fed into the flow network, which sends the flow resolution to the super-resolution network, SRNet, to be combined with the actual current low-resolution frame to be upscaled. The SRNet then produces a super-resolved high-resolution output.</p> <p>The loss functions used in this project include per-pixel loss, perception loss, total variation loss, and flow loss. By experimenting with different weight parameters and RAFT models, I was able to fine-tune the network for improved performance.</p> <h4 id="loss-functions">Loss Functions</h4> <p>In this project, a combination of various loss functions was utilized to optimize the performance of the SRNet model.</p> <p>1. Per-Pixel Loss: This loss function, calculated using Mean Squared Error (MSE), measures the low-level content differences between the super-resolved output and the ground truth high-resolution image. The equation for per-pixel loss is:</p> \[L_{\text{per-pixel}} = \sum_{i=1}^{n}(I_{\text{upscaled}}-I_{\text{ground truth}})^2\] <p>2. Perception Loss: This loss function captures high-level content similarities by measuring the difference in feature representation between the upscaled output and the ground truth image. The equation for perception loss is:</p> \[L_{\text{perception}} = \sum_{i=1}^{n}(F_{\text{upscaled}}-F_{\text{ground truth}})^2\] <p>3. Total Variation Loss: To reduce noise in the output, this loss function measures the integral of the absolute image gradient under the assumption that an image with significant noise will have a high-valued integral. The equation for total variation loss is:</p> \[L_{\text{total variation}} = \sum_{i, j}\sqrt{|I_{i+1,j} - I_{i,j}|^2 + |y_{i,j+1}-y_{i,j}|^2}\] <p>4. Flow Loss: To maintain temporal stability between frames, this loss function calculates the Mean Squared Error per-pixel between the predicted high-resolution frame morphed from the flow estimation and the current ground truth low-resolution frame. The equation for flow loss is:</p> \[L_{flow} = \sum_{i=1}^{n}(I_{\text{flow morphed}}-I_{\text{ground truth low res}})^2\] <p>5. Total Loss: The overall loss function is a combination of the aforementioned losses and is used to optimize the entire network. For a single video in the training set, the loss is averaged across all frames. The equation for total loss is:</p> \[L_{\text{total video}} = \sum_{i=1}^{n}(L_{\text{frame n}})\] <p>These loss functions work together to create a more accurate and visually pleasing super-resolution output while preserving both low and high-level content similarities and maintaining temporal stability.</p> <h4 id="training">Training</h4> <p>During the training phase of this project, the Vimeo90k dataset, specifically designed for image super-resolution, was utilized. The dataset consists of 7,824 videos, each containing 7 frames. Both high-resolution (448 x 256 pixels) and low-resolution (112 x 64 pixels) versions of the videos were provided. To simplify the process, low-resolution images were upscaled to the output resolution using Bilinear interpolation before being run through the network and compared to their high-resolution counterparts.</p> <p>Due to resource limitations, the dataset used for training was restricted to a subset of 2,500 videos, with a 20% validation split. The model was trained on an RTX 2070 Super Max-Q laptop GPU, with each of the 10 epochs taking approximately one hour to complete. This resource-conscious approach allowed for efficient model training while still yielding impressive results.</p> <h4 id="results-and-conclusion">Results and Conclusion</h4> <div class="container"> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/lowres-480.webp 480w,/assets/img/vsr_6869/lowres-800.webp 800w,/assets/img/vsr_6869/lowres-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/lowres.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Low Res Frame" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/bicubic-480.webp 480w,/assets/img/vsr_6869/bicubic-800.webp 800w,/assets/img/vsr_6869/bicubic-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/bicubic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Low Res Frame" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/groundtruth-480.webp 480w,/assets/img/vsr_6869/groundtruth-800.webp 800w,/assets/img/vsr_6869/groundtruth-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/groundtruth.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Ground Truth Frame" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> First Image: A frame from a low resolution video. Second Image: The frame upscaled using traditional bicubic interpolation. Third Image: The ground truth frame. </div> <div class="container"> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/FRVSR-480.webp 480w,/assets/img/vsr_6869/FRVSR-800.webp 800w,/assets/img/vsr_6869/FRVSR-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/FRVSR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Upscaled Frame using FRVSR" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/RAFRVSR-480.webp 480w,/assets/img/vsr_6869/RAFRVSR-800.webp 800w,/assets/img/vsr_6869/RAFRVSR-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/RAFRVSR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Upscaled Frame using my Model" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> </div> <div class="caption"> First Image: The frame upscaled using the original FRVSR. Second Image: The frame upscaled using my new model. </div> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/FRVSR-480.webp 480w,/assets/img/vsr_6869/FRVSR-800.webp 800w,/assets/img/vsr_6869/FRVSR-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/FRVSR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/vsr_6869/RAFRVSR-480.webp 480w,/assets/img/vsr_6869/RAFRVSR-800.webp 800w,/assets/img/vsr_6869/RAFRVSR-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/vsr_6869/RAFRVSR.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> First Image: The frame upscaled using the original FRVSR. Second Image: The frame upscaled using my new model. </div> <p>To evaluate the results, I focused on qualitative details, peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM). The improved network produced visually pleasing images with finer texture detail and sharper edges compared to the original FRVSR. However, it also suffered from amplifying noise present in the videos, causing the average PSNR and SSIM to be lower than expected.</p> <p>Despite this drawback, the modified network shows promise in producing visually pleasing results compared to the original FRVSR when trained on the same dataset and with the same hyperparameters. Further improvements can be made by refining the loss functions, using more robust datasets, and increasing the dataset size, batch size, and number of epochs trained. For this project in particular, due to limited GPU resources and time constraints, the number of epochs trained during the project was lower than desired. This impacted the overall performance of the model, as it could have potentially improved further with additional training time.</p> <p>In conclusion, this project has demonstrated the potential of enhancing video super-resolution using advanced neural networks and incorporating perceptual loss and RAFT. With further research and optimization, these techniques can be applied in a wide range of applications to significantly improve video quality.</p> <h4 id="videos">Videos</h4> <p>Sample input and output videos are available below.</p> <center> <iframe src="https://drive.google.com/file/d/1ERXi6uvwsjW-di5ctiRw9Iwj3d_6e1kW/preview" width="704" height="480" allow="autoplay"></iframe> <div class="caption"> Original, ground truth video </div> </center> <center> <iframe src="https://drive.google.com/file/d/1LBb0chE7gI2G5M3_6PNXoxNe1O1yvmmm/preview" width="704" height="480" allow="autoplay"></iframe> <div class="caption"> Low resolution downsampled input video </div> </center> <center> <iframe src="https://drive.google.com/file/d/1sDeFrSPj1802UImI0X7FiDc1UWnPUTAi/preview" width="704" height="480" allow="autoplay"></iframe> <div class="caption"> Video upscaled using traditional bicubic interpolation </div> </center> <center> <iframe src="https://drive.google.com/file/d/1kK_P5xkILrYNeEGdYKOkeAlZ1KkYvnPJ/preview" width="704" height="480" allow="autoplay"></iframe> <div class="caption"> Video upscaled using original FRVSR </div> </center> <center> <iframe src="https://drive.google.com/file/d/1U12v0iP3Y_k5157q8uFU1gKviPLI9tV1/preview" width="704" height="480" allow="autoplay"></iframe> <div class="caption"> Video upscaled using my revised model </div> </center> <h4 id="sources">Sources</h4> <p>Mehdi S. M. Sajjadi, Raviteja Vemulapalli, and Matthew Brown. Frame-recurrent video super-resolution, 2018. <a href="https://arxiv.org/abs/1801.04590" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/1801.04590.</a></p> <p>Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020. <a href="https://arxiv.org/abs/2003.12039" rel="external nofollow noopener" target="_blank">https://arxiv.org/abs/2003.12039.</a></p> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2024 John Eastman. </div> </footer> <script src="/assets/libs/jquery/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="/assets/libs/mdb/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/libs/masonry/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="/assets/libs/imagesloaded/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="/assets/libs/medium_zoom/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="/assets/libs/mathjax/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="/assets/libs/polyfill/polyfill.min.js" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-SN525KWJ5N"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-SN525KWJ5N");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="/assets/libs/img-comparison-slider/index.min.js" integrity="sha256-EXHg3x1K4oIWdyohPeKX2ZS++Wxt/FRPH7Nl01nat1o=" crossorigin="anonymous"></script> <script defer src="/assets/libs/swiper/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>